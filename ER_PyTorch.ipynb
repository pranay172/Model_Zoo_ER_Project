{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ER PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPPHbItPwIVh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "439fe794-862c-49e0-a696-5daf4089b9e9"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "%matplotlib inline\n",
        "import cv2\n",
        "import os\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from mlxtend.image import extract_face_landmarks\n",
        "# importing all the required libraries\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from skimage.transform import rotate, warp\n",
        "from skimage.util import random_noise\n",
        "from skimage.filters import gaussian\n",
        "#Essential sklearn Functions\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "#Essential PyTorch libraries\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d \n",
        "from torch.nn import MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
        "from torch.optim import SGD"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2 to /root/mlxtend_data/shape_predictor_68_face_landmarks.dat.bz2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSwAgM1kz72t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e0b1f4b-1009-4a2d-cec3-4175b1e5a3eb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJ9ngcGgz_fi"
      },
      "source": [
        "def eye_centers(landmarks):\n",
        "    '''\n",
        "    To find the eye centers(mean of the 6 landmark points around the eye)\n",
        "    '''\n",
        "    #36-41 are landmark points surrounding right eye\n",
        "    point1 = (np.mean(landmarks[36:42,0]),np.mean(landmarks[36:42,1]))\n",
        "    #42-47 are landmark points surrounding left eye\n",
        "    point2 = (np.mean(landmarks[42:48,0]),np.mean(landmarks[42:48,1]))\n",
        "\n",
        "    return point1, point2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iH9Hn4gk0BAQ"
      },
      "source": [
        "def find_angle(point1, point2):\n",
        "    '''\n",
        "    To find angle in degrees for the given two points\n",
        "    '''\n",
        "    # angle in radians\n",
        "    angle_r = math.atan((point2[1] - point1[1])/(point2[0] - point1[0]))\n",
        "    # angle in degrees\n",
        "    angle_d = math.degrees(angle_r)\n",
        "\n",
        "    return angle_d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZtrq5UI0ERn"
      },
      "source": [
        "def rotate_image(image,angle):\n",
        "    '''\n",
        "    Returns a rotated image given the angle(degrees) to be rotated and image\n",
        "    '''\n",
        "    rows,cols = image.shape\n",
        "    #Transformation Matrix(M)\n",
        "    M = cv2.getRotationMatrix2D(((rows - 1)/2.0,(cols - 1)/2.0),angle,1)\n",
        "    rot_img = cv2.warpAffine(image, M, (cols,rows))\n",
        "\n",
        "    return rot_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQWWXR7I0Imo"
      },
      "source": [
        "def preprocessing(image_data):\n",
        "\n",
        "    '''\n",
        "    The preprocessing involves rotation of the image and image cropping\n",
        "    Arguments :\n",
        "    image_data -- array of images of shape (m,h,w)\n",
        "    Returns :\n",
        "    array of images after preprocessing\n",
        "    '''\n",
        "    preprocessed_faces = []\n",
        "    for img in image_data:\n",
        "        \n",
        "        #landmark detection\n",
        "        #(returns an array of landmarks of shape (68,2))\n",
        "        landmarks = extract_face_landmarks(img)\n",
        "\n",
        "        #detect eye cnters\n",
        "        p1, p2 = eye_centers(landmarks)\n",
        "\n",
        "        #find angle \n",
        "        angle = find_angle(p1, p2)\n",
        "        \n",
        "        #rotate image\n",
        "        rot_img = rotate_image(img, angle)\n",
        "\n",
        "        #find length 'd'\n",
        "        p1_new, p2_new = eye_centers(extract_face_landmarks(rot_img))\n",
        "        d = cv2.norm(np.array(p1_new) - np.array(p2_new))\n",
        "\n",
        "        #mid point of new eye centers\n",
        "        d_mid = ((p2_new[0]+p1_new[0])/2.0,(p2_new[1]+p1_new[1])/2.0)\n",
        "\n",
        "        #point above line joining eye centers\n",
        "        x_up = d_mid[0]\n",
        "        y_up = d_mid[1] - (0.6*d)\n",
        "\n",
        "        #cropping image\n",
        "        x_start = int(landmarks[0,0])\n",
        "        x_end = int(landmarks[16,0])\n",
        "        y_start = int(y_up)\n",
        "        y_end = int(landmarks[8,1])\n",
        "\n",
        "        crop_img = img[y_start:y_end,x_start:x_end]\n",
        "\n",
        "        #resize the cropped image\n",
        "        face_roi = cv2.resize(crop_img,(48,48))\n",
        "        \n",
        "        preprocessed_faces.append(face_roi)\n",
        "\n",
        "    return np.array(preprocessed_faces)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIT_h0mr0ZJ3"
      },
      "source": [
        "def normalization(imagedata, mean, std_dev):\n",
        "    '''\n",
        "    To apply Histogram equalization and \n",
        "    Z-Square Normalization to the preprocessed images\n",
        "    Arguments :\n",
        "    imagedata -- array of preprocessed images of shape (m,h,w)\n",
        "    mean -- mean of imagedata array\n",
        "    std_dev -- standard deviation of imagedata array\n",
        "    Returns :\n",
        "    array of normalized images of shape (m,48,48)\n",
        "    '''\n",
        "    normalized_images = []\n",
        "    for i in range(imagedata.shape[0]):\n",
        "        #Histogram Equalization\n",
        "        hist_eqv = cv2.equalizeHist(imagedata[i])\n",
        "\n",
        "        #Z-Square normalization\n",
        "        zsq_norm = ((hist_eqv - mean)/std_dev)\n",
        "\n",
        "        #Resize\n",
        "        resized_image = cv2.resize(zsq_norm, (48,48))\n",
        "        normalized_images.append(resized_image)\n",
        "\n",
        "    return np.array(normalized_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNoxHgPF0m1x"
      },
      "source": [
        "def load_and_preprocess_data(dataset,num_classes,jaffe_dir_path = None,ck_dir_path = None):\n",
        "    '''\n",
        "    Loads the dataset given and also preprocesses it\n",
        "    '''\n",
        "    jaffe_data_list = []\n",
        "    jaffe_labels_list = []\n",
        "    ck_data_list = []\n",
        "    ck_labels_list = []\n",
        "\n",
        "    if dataset == 'jaffe' or dataset == 'combined':\n",
        "        \n",
        "        express_code = ['HA','AN','DI','FE','SA','SU','NE']\n",
        "        for img in os.listdir(jaffe_dir_path):\n",
        "\n",
        "            label = img[3:5]\n",
        "            if num_classes == 6 and label == 'NE':\n",
        "                continue                                              \n",
        "            read_img = cv2.imread(jaffe_dir_path+img,\n",
        "                                  cv2.IMREAD_GRAYSCALE)\n",
        "            jaffe_data_list.append(read_img)\n",
        "            jaffe_labels_list.append(express_code.index(label))\n",
        "\n",
        "        jaffe_data = np.array(jaffe_data_list)\n",
        "        jaffe_labels = np.array(jaffe_labels_list)\n",
        "        jaffe_preprocessed_data = preprocessing(jaffe_data)\n",
        "    if dataset == 'ck+' or dataset == 'combined':\n",
        "        \n",
        "        express_code = ['happy','anger','disgust','fear','sadness',\n",
        "                        'surprise','contempt']\n",
        "        for emcode in os.listdir(ck_dir_path):\n",
        "\n",
        "            if num_classes == 6 and emcode == 'contempt':\n",
        "                continue\n",
        "            lst = os.listdir(ck_dir_path + emcode + '/')\n",
        "            lst.sort()\n",
        "            for i in range(2,len(lst),3):\n",
        "                read_img = cv2.imread(ck_dir_path+emcode+'/'+lst[i],\n",
        "                                      cv2.IMREAD_GRAYSCALE)\n",
        "                ck_data_list.append(read_img)\n",
        "                ck_labels_list.append(express_code.index(emcode))\n",
        "\n",
        "        ck_data = np.array(ck_data_list)\n",
        "        ck_labels = np.array(ck_labels_list)\n",
        "        ck_preprocessed_data = preprocessing(ck_data)\n",
        "\n",
        "    print('Data loading and Preprocessing is completed')\n",
        "\n",
        "    if dataset == 'combined':\n",
        "        X = np.concatenate((jaffe_preprocessed_data,ck_preprocessed_data),axis = 0)\n",
        "        Y = np.concatenate((jaffe_labels,ck_labels),axis = 0)\n",
        "        preprocessed_data, labels = shuffle(X,Y)\n",
        "    elif dataset == 'ck+':\n",
        "        preprocessed_data, labels = shuffle(ck_preprocessed_data, ck_labels)\n",
        "    elif dataset == 'jaffe':\n",
        "        preprocessed_data, labels = shuffle(jaffe_preprocessed_data, jaffe_labels)\n",
        "    mean, std_dev = preprocessed_data.mean(),preprocessed_data.std()\n",
        "    normalized_data = normalization(preprocessed_data, mean, std_dev)\n",
        "    X_tmp = normalized_data.reshape(normalized_data.shape + (1,))\n",
        "    #Y_tmp = to_categorical(labels)\n",
        "    Y_tmp = labels\n",
        "    print(X_tmp.shape)\n",
        "    print(Y_tmp.shape)\n",
        "    print('Normalization and Datareshaping is completed')\n",
        "\n",
        "    return X_tmp,Y_tmp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVQR-Daq1rCa",
        "outputId": "0009c3a8-541f-45b0-d932-7e8ed2a1412b"
      },
      "source": [
        "X_tmp, Y_tmp = load_and_preprocess_data('ck+',7,None,'drive/My Drive/Dataset Images/CK+48/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data loading and Preprocessing is completed\n",
            "(327, 48, 48, 1)\n",
            "(327,)\n",
            "Normalization and Datareshaping is completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lh4ubBLc_MQg",
        "outputId": "21ed1326-2695-47c2-86fc-f1fed91deb95"
      },
      "source": [
        "train_x, val_x, train_y, val_y = train_test_split(X_tmp, Y_tmp, test_size = 0.1, random_state = 6, shuffle = True)\n",
        "print(train_x.shape, train_y.shape)\n",
        "print(val_x.shape, val_y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(294, 48, 48, 1) (294,)\n",
            "(33, 48, 48, 1) (33,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGj3KEtuEYRq",
        "outputId": "bd0a41fe-f4a6-4676-8bb7-562f813ca57f"
      },
      "source": [
        "X_train = []\n",
        "Y_train = []\n",
        "for i in tqdm(range(train_x.shape[0])):\n",
        "    X_train.append(train_x[i])\n",
        "    X_train.append(rotate(train_x[i], angle=3))\n",
        "    X_train.append(np.fliplr(train_x[i]))\n",
        "    X_train.append(random_noise(train_x[i],var=0.2**2))\n",
        "    for j in range(4):\n",
        "        Y_train.append(train_y[i])\n",
        "\n",
        "print('\\n', len(Y_train), len(X_train))\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "print(X_train.shape, Y_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 294/294 [00:00<00:00, 2408.87it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 1176 1176\n",
            "(1176, 48, 48, 1) (1176,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th5nRQ64JZhH"
      },
      "source": [
        "# converting training & Validation images, labels into torch format\n",
        "X_train = X_train.reshape(X_train.shape[0], 1, 48, 48)\n",
        "X_train  = torch.from_numpy(X_train)\n",
        "X_train = X_train.float()\n",
        "\n",
        "Y_train = Y_train.astype(int)\n",
        "Y_train = torch.from_numpy(Y_train)\n",
        "\n",
        "val_x = val_x.reshape(val_x.shape[0], 1, 48, 48)\n",
        "val_x  = torch.from_numpy(val_x)\n",
        "val_x = val_x.float()\n",
        "\n",
        "val_y = val_y.astype(int)\n",
        "val_y = torch.from_numpy(val_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOJLlk3vLwH4"
      },
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "class Net(Module):   \n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.cnn_layers = Sequential(\n",
        "            \n",
        "            # Defining 1st 2D convolution layer\n",
        "            Conv2d(1, 48, kernel_size=5, stride=1, padding=0),\n",
        "            ReLU(inplace=True),\n",
        "            BatchNorm2d(48),\n",
        "            MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
        "\n",
        "            # Defining 2nd 2D convolution layer\n",
        "            Conv2d(48, 64, kernel_size=5, stride=1, padding=0),\n",
        "            ReLU(inplace=True),\n",
        "            BatchNorm2d(64),\n",
        "            MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
        "        )\n",
        "\n",
        "        self.linear_layers = Sequential(\n",
        "            Dropout(p=0.5),\n",
        "            Linear(64 * 9 * 9, 7),\n",
        "        )\n",
        "\n",
        "    # Defining the forward pass    \n",
        "    def forward(self, x):\n",
        "        x = self.cnn_layers(x)\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = self.linear_layers(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpWfUiw2sSlU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd9fefec-f969-49fc-b12d-56b320a13fe1"
      },
      "source": [
        "model = Net()\n",
        "optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "criterion = CrossEntropyLoss()\n",
        "\n",
        "# checking if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    criterion = criterion.cuda()\n",
        "    \n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (cnn_layers): Sequential(\n",
            "    (0): Conv2d(1, 48, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (linear_layers): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=5184, out_features=7, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92GKhW0ZVm-H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6b6397f-5519-4320-d497-4e79cd882cc4"
      },
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# batch size\n",
        "batch_size = 16\n",
        "\n",
        "# (#epochs)\n",
        "n_epochs = 100\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "\n",
        "    train_loss = 0.0\n",
        "\n",
        "    #random permutation of training indices\n",
        "    permutation = torch.randperm(X_train.size()[0])\n",
        "\n",
        "    training_loss = []\n",
        "    for i in tqdm(range(0,X_train.size()[0], batch_size)):\n",
        "        #forming batches\n",
        "        indices = permutation[i:i+batch_size]\n",
        "        batch_x, batch_y = X_train[indices], Y_train[indices]\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            batch_x, batch_y = batch_x.cuda(), batch_y.cuda()\n",
        "        \n",
        "        optimizer.zero_grad() #making the parameters zero before training\n",
        "        outputs = model(batch_x)\n",
        "        loss = criterion(outputs,batch_y) #criterion used is CrossEntropy\n",
        "\n",
        "        training_loss.append(loss.item())\n",
        "        loss.backward() #backpropagation\n",
        "        optimizer.step() #Does the update\n",
        "        \n",
        "    training_loss = np.average(training_loss)\n",
        "    print('epoch: \\t', epoch, '\\t training loss: \\t', training_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 272.38it/s]\n",
            " 41%|████      | 30/74 [00:00<00:00, 298.28it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 1 \t training loss: \t 0.759352172448023\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 337.57it/s]\n",
            " 53%|█████▎    | 39/74 [00:00<00:00, 388.16it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 2 \t training loss: \t 0.2876296406989363\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 368.09it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 390.95it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 3 \t training loss: \t 0.21411659015418105\n",
            "epoch: \t 4 \t training loss: \t 0.14768997880683682\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 377.23it/s]\n",
            " 55%|█████▌    | 41/74 [00:00<00:00, 403.59it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 5 \t training loss: \t 0.09882556929684093\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 375.38it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 392.51it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 6 \t training loss: \t 0.04211037533103671\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 74/74 [00:00<00:00, 397.57it/s]\n",
            "  0%|          | 0/74 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 7 \t training loss: \t 0.028505464354246803\n",
            "epoch: \t 8 \t training loss: \t 0.02194259026637647\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 386.55it/s]\n",
            " 54%|█████▍    | 40/74 [00:00<00:00, 399.05it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 9 \t training loss: \t 0.012811293725905204\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 353.93it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 399.74it/s]\n",
            "  0%|          | 0/74 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 10 \t training loss: \t 0.01060048974220597\n",
            "epoch: \t 11 \t training loss: \t 0.011667508111713687\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 370.67it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 390.02it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 12 \t training loss: \t 0.0051504395241126245\n",
            "epoch: \t 13 \t training loss: \t 0.013393669893392487\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 387.62it/s]\n",
            " 54%|█████▍    | 40/74 [00:00<00:00, 395.71it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 14 \t training loss: \t 0.005787539412660801\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 368.33it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 395.55it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 15 \t training loss: \t 0.003601446796253456\n",
            "epoch: \t 16 \t training loss: \t 0.005880169985126436\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 393.64it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 385.54it/s]\n",
            "  0%|          | 0/74 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 17 \t training loss: \t 0.00847316838814697\n",
            "epoch: \t 18 \t training loss: \t 0.009835803450137924\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 371.75it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 388.35it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 19 \t training loss: \t 0.0117569088450526\n",
            "epoch: \t 20 \t training loss: \t 0.006848553393718101\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 375.47it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 390.60it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 21 \t training loss: \t 0.0017799464778756767\n",
            "epoch: \t 22 \t training loss: \t 0.0013431621279311446\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 388.86it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 377.50it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 23 \t training loss: \t 0.001687867159931999\n",
            "epoch: \t 24 \t training loss: \t 0.0027383025701756443\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 389.81it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 395.70it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 25 \t training loss: \t 0.002099395292957893\n",
            "epoch: \t 26 \t training loss: \t 0.001494054557497293\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 396.91it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 385.89it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 27 \t training loss: \t 0.000994034941942192\n",
            "epoch: \t 28 \t training loss: \t 0.0019152553036292375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 396.16it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 391.44it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 29 \t training loss: \t 0.0051257892165341296\n",
            "epoch: \t 30 \t training loss: \t 0.0019710583409923363\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 381.80it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 396.38it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 31 \t training loss: \t 0.0022601892685727492\n",
            "epoch: \t 32 \t training loss: \t 0.0011805779244536392\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 385.85it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 394.78it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 33 \t training loss: \t 0.002034668548251805\n",
            "epoch: \t 34 \t training loss: \t 0.002147439357283499\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 368.23it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 392.56it/s]\n",
            "  0%|          | 0/74 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 35 \t training loss: \t 0.001216237714712603\n",
            "epoch: \t 36 \t training loss: \t 0.002978957641357123\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 379.83it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 395.63it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 37 \t training loss: \t 0.0024308472171020875\n",
            "epoch: \t 38 \t training loss: \t 0.0027559594223942907\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 395.47it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 387.25it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 39 \t training loss: \t 0.00083112708106197\n",
            "epoch: \t 40 \t training loss: \t 0.0010239709741236348\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 372.18it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 384.58it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 41 \t training loss: \t 0.0006947651546348376\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 74/74 [00:00<00:00, 392.51it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 379.67it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 42 \t training loss: \t 0.0006995074251323318\n",
            "epoch: \t 43 \t training loss: \t 0.001114692487431979\n",
            "epoch: \t 44 \t training loss: \t 0.0003866487138566299\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 378.89it/s]\n",
            " 53%|█████▎    | 39/74 [00:00<00:00, 383.42it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 45 \t training loss: \t 0.0008921122586331409\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 377.21it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 396.13it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 46 \t training loss: \t 0.0005604442545776495\n",
            "epoch: \t 47 \t training loss: \t 0.0010176788722969217\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 384.11it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 382.08it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 48 \t training loss: \t 0.0013711489675950212\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 74/74 [00:00<00:00, 379.18it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 397.42it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 49 \t training loss: \t 0.0008812703579900011\n",
            "epoch: \t 50 \t training loss: \t 0.0012845247421827898\n",
            "epoch: \t 51 \t training loss: \t 0.0013167750071875776\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 379.19it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 393.80it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 52 \t training loss: \t 0.0005095777662272421\n",
            "epoch: \t 53 \t training loss: \t 0.0031284258268135237\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 389.21it/s]\n",
            " 53%|█████▎    | 39/74 [00:00<00:00, 385.40it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 54 \t training loss: \t 0.0013697003029748786\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 380.35it/s]\n",
            " 54%|█████▍    | 40/74 [00:00<00:00, 397.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 55 \t training loss: \t 0.0022600398656507153\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 374.38it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 396.04it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 56 \t training loss: \t 0.0008366748486485056\n",
            "epoch: \t 57 \t training loss: \t 0.0008130272284983516\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 389.53it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 388.75it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 58 \t training loss: \t 0.0007648580668083505\n",
            "epoch: \t 59 \t training loss: \t 0.0006007439393135575\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 378.14it/s]\n",
            " 53%|█████▎    | 39/74 [00:00<00:00, 381.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 60 \t training loss: \t 0.0008423116333792554\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 372.48it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 389.62it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 61 \t training loss: \t 0.000686850870474823\n",
            "epoch: \t 62 \t training loss: \t 0.0007582082381731738\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 386.84it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 398.07it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 63 \t training loss: \t 0.000619819883326813\n",
            "epoch: \t 64 \t training loss: \t 0.0004512129738007364\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 391.78it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 394.56it/s]\n",
            "  0%|          | 0/74 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 65 \t training loss: \t 0.000520301493438453\n",
            "epoch: \t 66 \t training loss: \t 0.00028552450759115026\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 374.35it/s]\n",
            " 47%|████▋     | 35/74 [00:00<00:00, 342.44it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 67 \t training loss: \t 0.00029735006373320593\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 351.97it/s]\n",
            " 51%|█████▏    | 38/74 [00:00<00:00, 373.58it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 68 \t training loss: \t 0.00033956388665501436\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 377.40it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 397.13it/s]\n",
            "  0%|          | 0/74 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 69 \t training loss: \t 0.000435968249594471\n",
            "epoch: \t 70 \t training loss: \t 0.001366450384896755\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 383.56it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 394.55it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 71 \t training loss: \t 0.0004980594014980581\n",
            "epoch: \t 72 \t training loss: \t 0.00040237313106826077\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 388.44it/s]\n",
            " 54%|█████▍    | 40/74 [00:00<00:00, 393.18it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 73 \t training loss: \t 0.000304654708899663\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 377.44it/s]\n",
            " 54%|█████▍    | 40/74 [00:00<00:00, 392.27it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 74 \t training loss: \t 0.00038998548055477276\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 362.35it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 395.28it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 75 \t training loss: \t 0.0007032692459983377\n",
            "epoch: \t 76 \t training loss: \t 0.0001831803545864868\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 386.25it/s]\n",
            " 53%|█████▎    | 39/74 [00:00<00:00, 386.02it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 77 \t training loss: \t 0.00023858786643223858\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 379.45it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 384.54it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 78 \t training loss: \t 0.0006404673391824798\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 74/74 [00:00<00:00, 389.35it/s]\n",
            " 51%|█████▏    | 38/74 [00:00<00:00, 375.93it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 79 \t training loss: \t 0.0007999149070938811\n",
            "epoch: \t 80 \t training loss: \t 0.0006367596769937766\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 369.16it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 392.84it/s]\n",
            "  0%|          | 0/74 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 81 \t training loss: \t 0.0002504406829480964\n",
            "epoch: \t 82 \t training loss: \t 0.0005397322772628162\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 362.61it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 385.75it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 83 \t training loss: \t 0.0003471694361094804\n",
            "epoch: \t 84 \t training loss: \t 0.0006858011895597097\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 379.93it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 385.87it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 85 \t training loss: \t 0.000422275003293057\n",
            "epoch: \t 86 \t training loss: \t 0.0016600708268138201\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 377.10it/s]\n",
            " 53%|█████▎    | 39/74 [00:00<00:00, 389.34it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 87 \t training loss: \t 0.0010758771622378747\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 378.50it/s]\n",
            " 50%|█████     | 37/74 [00:00<00:00, 369.77it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 88 \t training loss: \t 0.0002883466082998033\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 371.30it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 385.35it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 89 \t training loss: \t 0.0002691420986532132\n",
            "epoch: \t 90 \t training loss: \t 0.00034326086764652956\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 391.84it/s]\n",
            " 51%|█████▏    | 38/74 [00:00<00:00, 377.14it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 91 \t training loss: \t 0.0003901843867633697\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 354.39it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 374.69it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 92 \t training loss: \t 0.00020727238240175898\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 74/74 [00:00<00:00, 388.39it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 383.93it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 93 \t training loss: \t 0.00042002564157735687\n",
            "epoch: \t 94 \t training loss: \t 0.0006194793830869654\n",
            "epoch: \t 95 \t training loss: \t 0.00034546575149171395\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 364.39it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 380.59it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 96 \t training loss: \t 0.0002508604820071788\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 74/74 [00:00<00:00, 390.79it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 383.62it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 97 \t training loss: \t 0.00040679827766571425\n",
            "epoch: \t 98 \t training loss: \t 0.00019536184404571672\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 55%|█████▌    | 41/74 [00:00<00:00, 400.67it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 99 \t training loss: \t 0.000443612160361718\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 74/74 [00:00<00:00, 388.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: \t 100 \t training loss: \t 0.0003366751499734656\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6EkJ6JwYnpl"
      },
      "source": [
        "torch.save(model, 'model.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdDhPfQgYo75"
      },
      "source": [
        "the_model = torch.load('model.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-kkHCypYsBb",
        "outputId": "99db5b4e-2364-473c-b994-9ea02ade10d1"
      },
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# prediction for training set\n",
        "prediction = []\n",
        "target = []\n",
        "\n",
        "permutation = torch.randperm(X_train.size()[0])\n",
        "\n",
        "for i in tqdm(range(0,X_train.size()[0], batch_size)):\n",
        "    \n",
        "    indices = permutation[i:i+batch_size]\n",
        "    batch_x, batch_y = X_train[indices], Y_train[indices]\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        batch_x, batch_y = batch_x.cuda(), batch_y.cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(batch_x.cuda())\n",
        "\n",
        "    softmax = torch.exp(output).cpu()\n",
        "    prob = list(softmax.numpy()) #list of probabilities\n",
        "    predictions = np.argmax(prob, axis=1) #label with highest prob\n",
        "    prediction.append(predictions)\n",
        "    target.append(batch_y)\n",
        "    \n",
        "# training accuracy\n",
        "accuracy = []\n",
        "for i in range(len(prediction)):\n",
        "    accuracy.append(accuracy_score(target[i].cpu(),prediction[i]))\n",
        "    \n",
        "print('training accuracy: \\t', np.average(accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:00<00:00, 789.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training accuracy: \t 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUZm3b9wZMrz",
        "outputId": "35b20c76-d4ca-4a80-8253-a4ef0c7a1013"
      },
      "source": [
        "#val accuracy\n",
        "torch.manual_seed(42)\n",
        "output = model(val_x.cuda())\n",
        "\n",
        "softmax = torch.exp(output).cpu()\n",
        "prob = list(softmax.detach().numpy())\n",
        "predictions = np.argmax(prob, axis=1)\n",
        "accuracy_score(val_y, predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9090909090909091"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luh11AhuBNao"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}